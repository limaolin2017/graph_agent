# ü§ñ Web Testing Automation Agent (2025 Edition)

Modern ReAct intelligent agent built with LangGraph, focused on web testing automation. Utilizes the latest PostgreSQL + PGVector technology stack with enterprise-grade code quality.

**‚úÖ Completed Features:**
- üß† Intelligent Agent Automation (LangGraph ReAct Architecture)
- üîç Advanced Semantic Search (PostgreSQL + PGVector + OpenAI embeddings)
- üíæ Data Persistence & LangGraph Checkpoints (2025 Standard)
- üåê Web Scraping and Test Code Generation
- üîß Clean Code Architecture (Zero Code Duplication)
- üìä Multi-turn Conversation Memory
- üéØ Experience-based Learning (RAG)
- üê≥ Docker Containerized Deployment
- üåê Gradio Web Interface + CLI Command Line Interface

## ‚ú® Core Features

### üß∞ Six Core Tools

1. **üåê scrape_url()** - Web Scraping
   - Extract HTML content from specified URLs
   - Supports Firecrawl integration for enhanced scraping capabilities
   - Returns structured page content and metadata

2. **üìã generate_requirements()** - Requirements Analysis  
   - Uses AI to analyze HTML content
   - Extracts functional requirements from page elements
   - Identifies forms, navigation, buttons, and interactive elements

3. **üß™ generate_test_code()** - Test Code Generation
   - Generates Cypress tests based on requirements
   - Supports Gherkin and JavaScript formats
   - Includes common UI interaction patterns

4. **üìä show_status()** - Evaluation Metrics Display
   - Fetches latest evaluation metrics from LangSmith
   - Displays metrics like step_completed from evaluator runs
   - Supports querying specific metrics by name

5. **üîç search_experience()** - Historical Experience Search
   - Performs semantic search on past test cases using PGVector
   - Utilizes previous work to avoid repetition
   - RAG-driven knowledge retrieval

6. **üîé search_artifacts()** - Specific Content Search
   - Searches for functional requirements and test code
   - Leverages key page elements for artifact discovery
   - Ensures relevant templates and scenarios are found

### üåê Dual Interface Support

- **Gradio Web Interface**: User-friendly graphical interface with real-time streaming responses
- **CLI Command Line Interface**: Command-line tool for advanced users

## üöÄ Quick Start

### Method 1: Docker Deployment (Recommended)

```bash
# 1. Clone the project
git clone <repository-url>
cd web-testing-agent

# 2. Configure environment variables
cp .env.example .env
# Edit .env file to set your OPENAI_API_KEY

# 3. Start all services
docker-compose up -d

# 4. Access the application
# Web interface: http://localhost:7861
# Database: localhost:5432
```

### Method 2: Local Installation

```bash
# 1. Install dependencies (2025 version)
pip install -r requirements.txt

# 2. Configure API keys
# Create .env file or set environment variables:
export OPENAI_API_KEY="your-openai-api-key"
export DATABASE_URL="postgresql://user:pass@localhost:5432/web_testing"
# Optional: Use Firecrawl for enhanced scraping
export FIRECRAWL_API_KEY="your-firecrawl-api-key"

# 3. Run the application
# Main program - Gradio Web interface (user-friendly interface)
python main.py

# Command line interface (advanced users)
python cli.py

# Test database connection and PGVector search
python tests/test_db.py

# Basic functionality test
python tests/simple_test.py
```

### CLI Interaction Commands
After running, you can use the following commands:
- `quit` or `q` - Exit the program
- `history` or `h` - View recent test run records
- `new` or `n` - Start a new conversation session
- `reset` or `r` - Reset the current conversation

## üí° Usage Examples

### 1. Complete Web Testing Workflow
```
"Help me test this website: https://example.com/login"
```
The agent will automatically:
1. Scrape webpage content
2. Generate functional requirements
3. Create Cypress test code
4. Display status and metrics

### 2. Individual Tool Usage
```
"Scrape the HTML of https://example.com/contact"
"Generate requirements for the scraped HTML"
"Generate Cypress test code in JavaScript format"
"Show current status"
```

### 3. Experience-based Learning
```
"Search for previous login form tests"
"Find similar e-commerce test examples"
```

### 4. Multi-turn Conversations
The agent remembers context across interactions:
```
User: "Scrape https://shop.example.com"
Agent: [Scrapes and analyzes the page]
User: "Now generate tests for the checkout process"
Agent: [Uses previous context to generate relevant tests]
```

### 5. Docker Environment Usage
```bash
# View service status
docker-compose ps

# View application logs
docker logs web_testing_agent

# Restart application (after code updates)
docker-compose restart agent

# Enter container for debugging
docker exec -it web_testing_agent bash
```

## üé¨ Actual Runtime Effects

### Web Interface Features
- **Real-time Streaming Responses**: See the agent's complete thought process
- **Tool Call Visualization**: Display each tool call and result
- **Multi-turn Conversation Memory**: Support for context-related continuous conversations
- **Multilingual Support**: Interface and responses support both Chinese and English

### Typical Workflow Demonstration
1. **User Input**: "Help me test the login functionality of https://example.com"
2. **Agent Thinking**: Displays "ü§î Thinking..."
3. **Tool Execution**: 
   - üîß **Executing**: scrape_url
   - ‚úÖ **Result**: Webpage content successfully scraped
   - üîß **Executing**: generate_requirements  
   - ‚úÖ **Result**: Functional requirements analysis completed
   - üîß **Executing**: generate_test_code
   - ‚úÖ **Result**: Cypress test code generated
4. **Final Output**: Complete test code and explanation

### CLI Interface Features
- **Session Management**: Supports multi-turn conversation memory
- **History Records**: View previous test runs
- **Database Integration**: Automatically saves all artifacts to PostgreSQL
- **Semantic Search**: Intelligent recommendations based on historical experience

## üìÅ Project Structure

```
‚îú‚îÄ‚îÄ main.py                 # Main entry point (Gradio Web interface + application bootstrap)
‚îú‚îÄ‚îÄ cli.py                  # Command line interface
‚îú‚îÄ‚îÄ config.py              # Configuration management (ConfigManager class)
‚îú‚îÄ‚îÄ requirements.txt        # Python dependencies (2025 version)
‚îú‚îÄ‚îÄ Dockerfile             # Docker image build file
‚îú‚îÄ‚îÄ docker-compose.yml     # Docker orchestration configuration
‚îú‚îÄ‚îÄ deploy.md              # Deployment guide and status
‚îú‚îÄ‚îÄ agent/                 # Intelligent agent module
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py        # Agent module exports
‚îÇ   ‚îú‚îÄ‚îÄ agent.py           # LangGraph ReAct agent creation
‚îÇ   ‚îú‚îÄ‚îÄ state.py           # Agent state pattern (TypedDict)
‚îÇ   ‚îú‚îÄ‚îÄ tools.py           # Five core tool implementations
‚îÇ   ‚îú‚îÄ‚îÄ utils.py           # Utility functions (scraping, test generation)
‚îÇ   ‚îî‚îÄ‚îÄ prompt.py          # System prompts and templates
‚îú‚îÄ‚îÄ database/              # Database module
‚îÇ   ‚îî‚îÄ‚îÄ __init__.py        # PostgreSQL + PGVector integration
‚îú‚îÄ‚îÄ tests/                 # Test module
‚îÇ   ‚îú‚îÄ‚îÄ simple_test.py     # Basic functionality test
‚îÇ   ‚îî‚îÄ‚îÄ test_db.py         # Database and semantic search test
‚îî‚îÄ‚îÄ README.md              # Project documentation
```

## üîß Core Architecture

### System Architecture Diagram

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                        Web Testing Agent System                      ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                     ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îÇ
‚îÇ  ‚îÇ   User Input    ‚îÇ         ‚îÇ    LangGraph ReAct Agent     ‚îÇ     ‚îÇ
‚îÇ  ‚îÇ  (Gradio/CLI)   ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ     ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îÇ  ‚îÇ   State Management     ‚îÇ  ‚îÇ     ‚îÇ
‚îÇ                              ‚îÇ  ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ  ‚îÇ     ‚îÇ
‚îÇ                              ‚îÇ  ‚îÇ  ‚îÇ  scraped_html    ‚îÇ  ‚îÇ  ‚îÇ     ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îÇ  ‚îÇ  ‚îÇ  requirements    ‚îÇ  ‚îÇ  ‚îÇ     ‚îÇ
‚îÇ  ‚îÇ   OpenAI API    ‚îÇ‚óÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ  ‚îÇ  ‚îÇ  test_code       ‚îÇ  ‚îÇ  ‚îÇ     ‚îÇ
‚îÇ  ‚îÇ  (GPT-4o/mini)  ‚îÇ         ‚îÇ  ‚îÇ  ‚îÇ  run_id          ‚îÇ  ‚îÇ  ‚îÇ     ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ  ‚îÇ     ‚îÇ
‚îÇ                              ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ     ‚îÇ
‚îÇ                              ‚îÇ                              ‚îÇ     ‚îÇ
‚îÇ                              ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ     ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îÇ  ‚îÇ       Six Tools        ‚îÇ  ‚îÇ     ‚îÇ
‚îÇ  ‚îÇ  External APIs  ‚îÇ‚óÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ  ‚îÇ ‚Ä¢ scrape_url          ‚îÇ  ‚îÇ     ‚îÇ
‚îÇ  ‚îÇ  (Firecrawl)    ‚îÇ         ‚îÇ  ‚îÇ ‚Ä¢ generate_requirements‚îÇ  ‚îÇ     ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îÇ  ‚îÇ ‚Ä¢ generate_test_code  ‚îÇ  ‚îÇ     ‚îÇ
‚îÇ                              ‚îÇ  ‚îÇ ‚Ä¢ show_status         ‚îÇ  ‚îÇ     ‚îÇ
‚îÇ                              ‚îÇ  ‚îÇ ‚Ä¢ search_experience   ‚îÇ  ‚îÇ     ‚îÇ
‚îÇ                              ‚îÇ  ‚îÇ ‚Ä¢ search_artifacts    ‚îÇ  ‚îÇ     ‚îÇ
‚îÇ                              ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ     ‚îÇ
‚îÇ                              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îÇ
‚îÇ                                           ‚îÇ                        ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ                  PostgreSQL + PGVector Database              ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ    Runs     ‚îÇ    ‚îÇ  Artifacts   ‚îÇ    ‚îÇVector Search  ‚îÇ  ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ   Table     ‚îÇ    ‚îÇ    Table     ‚îÇ    ‚îÇ   (HNSW)      ‚îÇ  ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ                                                                     ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ                     Docker Container                         ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ web_testing_agent (port 7861)                            ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ postgres with pgvector (port 5432)                       ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ pgadmin [optional] (port 8080)                           ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Agent Creation
```python
# Clean, modern agent setup
def get_agent(checkpointer=None):
    model = ChatOpenAI(**MODEL_CONFIG)
    checkpointer = checkpointer or InMemorySaver()
    
    return create_react_agent(
        model=model,
        tools=[scrape_url, generate_requirements, generate_test_code, 
               show_status, search_experience],
        prompt=AGENT_SYSTEM_PROMPT,
        checkpointer=checkpointer,
        state_schema=AgentState
    )
```

### Session Management
```python
class SessionManager:
    def __init__(self):
        self.current_thread_id: Optional[str] = None
        self.agent = None
    
    def get_or_create_session(self) -> Tuple[dict, object]:
        if not self.current_thread_id:
            self.current_thread_id = str(uuid.uuid4())
            self.agent = get_agent()
        return {"configurable": {"thread_id": self.current_thread_id}}, self.agent
```

### Database Integration
```python
# PostgreSQL + PGVector semantic search
def search_artifacts_advanced(query: str, k: int = 5) -> List[dict]:
    embeddings = get_embeddings()
    query_vector = embeddings.embed_query(query)
    # Use pgvector for vector similarity search
    return results
```

### Gradio Web Interface
```python
# Web interface supporting real-time streaming responses
def respond_stream(query, history, thread_id):
    """Main conversation handler with history persistence"""
    # Stream process agent responses, displaying thought process in real-time
    for step in GLOBAL_AGENT.stream(...):
        # Process tool calls and results
        yield history, history, thread_id
```

## üéØ Workflow

### Automated Toolchain
1. **Input URL** ‚Üí Agent calls `scrape_url(url)`
2. **Analyze Content** ‚Üí Agent calls `generate_requirements()` 
3. **Generate Tests** ‚Üí Agent calls `generate_test_code(format_type)`
4. **Display Results** ‚Üí Agent calls `show_status()`
5. **Learn from History** ‚Üí Agent uses `search_experience(query)` as needed

### State Management
- Tools share data through agent state (no manual parameter passing)
- HTML content flows from scraping to requirements generation
- Requirements flow to test code generation
- All artifacts are stored in PostgreSQL with vector embeddings

### Memory and Learning
- Multi-turn conversations maintain context
- Historical test cases can be searched via semantic similarity
- Previous solutions guide new test generation
- Experience accumulates over time for better results

### Docker Deployment Architecture
- **Application Container**: Runs Web Testing Agent
- **Database Container**: PostgreSQL + PGVector extension
- **Management Interface**: pgAdmin (optional)
- **Network**: Docker internal network ensures service communication
- **Volume Mounting**: Supports hot code updates

## üåü Core Features

- ‚úÖ **Clean Architecture**: Modular design with zero code duplication
- ‚úÖ **Intelligent Decision Making**: ReAct architecture with automatic tool selection
- ‚úÖ **Complete Workflow**: End-to-end process from webpage to test code
- ‚úÖ **Flexible Output**: Multiple test formats (Gherkin, JavaScript)
- ‚úÖ **Memory and Learning**: Multi-turn conversations with experience retention
- ‚úÖ **Semantic Search**: PGVector-driven historical knowledge retrieval
- ‚úÖ **Enterprise Ready**: Comprehensive error handling, logging, and configuration management
- ‚úÖ **Easy to Extend**: Template-based approach to add new test types
- ‚úÖ **Database Integration**: PostgreSQL with vector embedding persistence
- ‚úÖ **Containerized Deployment**: Docker support for one-click deployment
- ‚úÖ **Dual Interface Support**: Web interface + CLI command line
- ‚úÖ **Real-time Streaming Responses**: Visualization of agent thinking process
- ‚úÖ **Hot Code Updates**: Supports real-time code modifications in Docker environment

## üìö Technology Stack

### Core Frameworks
- **LangGraph**: Agent framework with state management
- **LangChain**: LLM application development toolkit
- **OpenAI GPT-4o**: Primary reasoning language model
- **OpenAI GPT-4o-mini**: Efficient model for requirements analysis

### Database and Search
- **PostgreSQL**: Main database for runs and artifacts
- **PGVector**: Vector similarity search for semantic retrieval
- **OpenAI Embeddings**: text-embedding-3-small for vector generation
- **psycopg3**: Modern PostgreSQL driver

### Web Scraping
- **Firecrawl**: Enhanced web scraping (optional)
- **Requests**: HTTP client for basic scraping
- **Markdown**: Content format for scraped pages

### User Interface
- **Gradio**: Modern web interface framework
- **Real-time Streaming Responses**: Supports visualization of agent thinking process
- **Multilingual Support**: Chinese and English interfaces

### Deployment and Operations
- **Docker**: Containerized deployment
- **Docker Compose**: Multi-service orchestration
- **pgAdmin**: Database management interface (optional)
- **Health Checks**: Service status monitoring

### Development and Testing
- **Python 3.11**: Modern Python with type hints
- **asyncio**: Asynchronous programming support
- **pytest**: Test framework (for future test expansion)

## üîÑ Latest Improvements (Code Cleanup)

### Architecture Enhancements
- **Session Management**: Replaced global variables with `SessionManager` class
- **Configuration Management**: Introduced `ConfigManager` for centralized configuration handling
- **Error Handling**: Standardized error patterns using helper functions
- **Template Organization**: Centralized test templates for improved maintainability

### Code Quality
- **Language Consistency**: Standardized to English throughout the codebase
- **Function Decomposition**: Broke down large functions into focused helper functions
- **Type Safety**: Added comprehensive type hints
- **Documentation**: Improved docstrings and inline comments

### Performance and Reliability
- **Database Operations**: Optimized PostgreSQL queries and connection handling
- **Vector Search**: Enhanced PGVector integration with fallback mechanisms
- **Memory Management**: Improved asynchronous processing and resource cleanup
- **Testing**: Enhanced test coverage and validation

### Dockerized Deployment
- **Containerization**: Full Docker support including multi-service orchestration
- **Hot Code Updates**: Supports real-time code modifications during development
- **Health Checks**: Automatic service status monitoring
- **Network Isolation**: Secure container-to-container communication

### User Experience
- **Real-time Streaming Responses**: Gradio interface supports visualization of agent thinking process
- **Multi-interface Support**: Web interface and CLI coexist
- **Chinese Localization**: Supports Chinese user interface and documentation

## üìä Database Schema

### Tables Structure

#### 1. **runs** - Test execution records
```sql
CREATE TABLE runs (
    run_id TEXT PRIMARY KEY,           -- Unique identifier (e.g., run_abc123)
    url TEXT NOT NULL,                 -- Target URL being tested
    start_ts TIMESTAMP DEFAULT NOW(),  -- Run start timestamp
    status TEXT DEFAULT 'running',     -- Status: running/completed/error
    user_id TEXT,                      -- Optional user identifier
    model TEXT DEFAULT 'gpt-4o',       -- LLM model used
    duration INTEGER,                  -- Execution duration in seconds
    description TEXT,                  -- Run description/purpose
    created_at TIMESTAMP DEFAULT NOW(),-- Record creation time
    updated_at TIMESTAMP DEFAULT NOW() -- Last update time
);
```

#### 2. **artifacts** - Test artifacts with semantic embeddings
```sql
CREATE TABLE artifacts (
    id TEXT PRIMARY KEY,               -- Unique identifier
    run_id TEXT NOT NULL,              -- Foreign key to runs table
    type TEXT NOT NULL,                -- Type: tool_call/tool_result/requirement/test_code
    text TEXT NOT NULL,                -- Artifact content
    embedding VECTOR(512),             -- 512-dimensional semantic embedding
    summary TEXT,                      -- AI-generated summary
    timestamp TIMESTAMP DEFAULT NOW(), -- Creation timestamp
    url TEXT,                          -- Associated URL
    
    CONSTRAINT fk_artifacts_run_id
        FOREIGN KEY (run_id) REFERENCES runs(run_id)
        ON DELETE CASCADE
);
```

### Indexes
- **Performance indexes**: `idx_runs_start_ts`, `idx_artifacts_run_id`, `idx_artifacts_type`
- **Vector similarity index**: `idx_artifacts_embedding` using HNSW algorithm

### Evaluation Metrics

The system tracks comprehensive evaluation metrics through the `show_status()` tool:

#### Core Performance Metrics
1. **step_completed** - Number of successfully completed workflow steps
2. **tool_calls** - Total number of tool invocations
3. **success_rate** - Percentage of successful operations
4. **execution_time** - Time taken for each operation
5. **error_count** - Number of errors encountered

#### Quality Assessment Evaluators
6. **workflow_completion** - Evaluates if full web testing pipeline was completed (scraping ‚Üí requirements ‚Üí tests)
7. **content_consistency** - Assesses whether generated artifacts accurately reflect actual webpage content
8. **requirements_accuracy** - Measures how well requirements capture real page features
9. **test_code_quality** - Evaluates generated test scenarios for executability and coverage

#### Advanced Evaluation Features
- **Anti-Hallucination Detection**: Identifies invented features not present in scraped content
- **Completeness Scoring**: Measures coverage of important page elements
- **Practical Utility Assessment**: Evaluates if outputs are actionable for real testing

Metrics are fetched from LangSmith evaluator runs and displayed in real-time during agent execution.

## üê≥ Docker Deployment Details

### Service Components
- **web_testing_agent**: Main application container (port 7861)
- **postgres**: PostgreSQL database + PGVector (port 5432)
- **pgadmin**: Database management interface (port 8080, optional)

### Deployment Commands
```bash
# Start all services
docker-compose up -d

# View service status
docker-compose ps

# View application logs
docker logs web_testing_agent

# Restart application (after code updates)
docker-compose restart agent

# Stop all services
docker-compose down
```

### Environment Configuration
Configure in `.env` file:
```bash
OPENAI_API_KEY=your-openai-api-key
FIRECRAWL_API_KEY=your-firecrawl-api-key  # optional
LANGSMITH_API_KEY=your-langsmith-key      # optional
LANGCHAIN_TRACING_V2=true                 # optional
```

## ü§ù Contributing

Contributions are welcome! Feel free to submit Issues and Pull Requests.

### Development Environment Setup
1. Clone the repository
2. Install dependencies: `pip install -r requirements.txt`
3. Set up PostgreSQL and PGVector extension
4. Configure environment variables
5. Run tests: `python tests/simple_test.py`

### Docker Development Environment
```bash
# Use Docker for development
docker-compose up -d
# Code modifications take effect automatically without rebuilding images
```

## üìÑ License

MIT License - See LICENSE file for details
